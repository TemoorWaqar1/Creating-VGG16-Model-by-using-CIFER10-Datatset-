{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "VGG16 Model - Temoor Waqar - Mini Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d1fe1aa461c94b8dbe6b3809e2a625c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c9aa8ee377364afea9cceae732097783",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8ad47e483af2421e87efa3ebcf12f3ca",
              "IPY_MODEL_fce7535fa920452f9d1a4477d2eda657"
            ]
          }
        },
        "c9aa8ee377364afea9cceae732097783": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8ad47e483af2421e87efa3ebcf12f3ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c8a1c636ee144623bee34ca81c994c8a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8b29b7a881304c41b78c427cbd35c11b"
          }
        },
        "fce7535fa920452f9d1a4477d2eda657": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_288b0681e6ce4563b7904789ae21586f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [38:53&lt;00:00, 73060.27it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a27196b7c396457cbc950696288fc68c"
          }
        },
        "c8a1c636ee144623bee34ca81c994c8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8b29b7a881304c41b78c427cbd35c11b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "288b0681e6ce4563b7904789ae21586f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a27196b7c396457cbc950696288fc68c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSV-czD7Bsbx"
      },
      "source": [
        "Implementation of VGG16 Model by using CIFER10 Datatset \n",
        "It is also called the OxfordNet model, named after the Visual Geometry Group from Oxford.\n",
        "Number 16 refers that it has a total of 16 layers that has some weights.\n",
        "It Only has Conv and pooling layers in it.\n",
        "Always use a 3 x 3 Kernel for convolution.\n",
        "2x2 size of the max pool.\n",
        "has a total of about 138 million parameters.\n",
        "Trained on ImageNet data\n",
        "It has an accuracy of 92.7%.\n",
        "it has one more version of it Vgg 19, a total of 19 layers with weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7JDnDPIqzG5"
      },
      "source": [
        "#Liberaries that are needed to build VGG16 model \n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import models\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFhlF4WYgqmy",
        "outputId": "910a7d83-0d24-4dca-9e76-6da0102e6c80"
      },
      "source": [
        "# check GPU availability\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "d1fe1aa461c94b8dbe6b3809e2a625c4",
            "c9aa8ee377364afea9cceae732097783",
            "8ad47e483af2421e87efa3ebcf12f3ca",
            "fce7535fa920452f9d1a4477d2eda657",
            "c8a1c636ee144623bee34ca81c994c8a",
            "8b29b7a881304c41b78c427cbd35c11b",
            "288b0681e6ce4563b7904789ae21586f",
            "a27196b7c396457cbc950696288fc68c"
          ]
        },
        "id": "Fa1DjTFL1BBu",
        "outputId": "55093922-2832-46de-fea0-22ca1596f1c2"
      },
      "source": [
        "# Here We need to convert dataset to tensor because tensor is efficient than numpy. compose function takes two parameters. tensor to convert dataset into array and normalize function to obtain good accuracy.\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.Resize((224, 224)),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# training dataset. root to save data in directory, train to train data, download to download data, tf to assign above transform function in training data\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "# DataLoader wraps an iterable around the Dataset to enable easy access to the samples. num_workers to increase the number of processes running simultaneously by allowing multiprocessing.\n",
        "\n",
        "# batch size takes 4 samples of dataset at a time to avoid high computations and to save time\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
        "                                          shuffle=True)\n",
        "\n",
        "#same procedure for testing data.\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
        "                                         shuffle=False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1fe1aa461c94b8dbe6b3809e2a625c4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biWkTOIyDTeW"
      },
      "source": [
        "**Preparing VGG16 model with with 5 layer block because VGG16 have 5 block of layers for training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOCZIjRs7BA7"
      },
      "source": [
        "\n",
        "\n",
        "#Defining Class which is Extending neuralnetwork(nn) module   \n",
        "class VGG16(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes):                  #Initializing constructor of class\n",
        "\n",
        "        #Initialize Super class\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        #Note: VGG16 has 5 blocks of layers that's why we are creating 5 layer blocks \n",
        "\n",
        "        #Creating layers for VGG16 model\n",
        "\n",
        "        #creating sequential container where convulational layer is designed.  Sequential method means that all the layers of the model will be arranged in sequence.\n",
        "        \n",
        "        \n",
        "        self.block_1 = torch.nn.Sequential(\n",
        "                torch.nn.Conv2d(in_channels=3,                #in_channels refers to input channels to convulutionallayer. Our image is in RGB form so input channels are 3\n",
        "                                out_channels=64,              #out_channels refers to output channels to convulutionallayer. Output channels are input of another layer and so on.\n",
        "                                kernel_size=(3, 3),           #The kernels in the convolutional layer, are the convolutional filters. The kernel size here refers to the widthxheight of the filter mask.          \n",
        "                                stride=(1, 1),                #Stride is a parameter of the neural network's filter that modifies the amount of movement over the image. here stride is 1 means image is move 1 pixel\n",
        "                                padding=1),                   #Padding refers to the amount of pixels added to an image when it is being processed by the kernel of a CNN. Here pixel value that is added is 1 pixel\n",
        "                torch.nn.ReLU(),                              #relu is non linear activation function which help to decide if the neuron would fire or not\n",
        "                torch.nn.Conv2d(in_channels=64,                 #same as above\n",
        "                                out_channels=64,\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(1, 1),\n",
        "                                padding=1),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.MaxPool2d(kernel_size=(2, 2),        #Max pool convolve most prominent features      \n",
        "                                   stride=(2, 2))\n",
        "        )\n",
        "        \n",
        "        self.block_2 = torch.nn.Sequential(                     #definig other blocks same as above to train model efficiently\n",
        "                torch.nn.Conv2d(in_channels=64,\n",
        "                                out_channels=128,\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(1, 1),\n",
        "                                padding=1),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.Conv2d(in_channels=128,\n",
        "                                out_channels=128,\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(1, 1),\n",
        "                                padding=1),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
        "                                   stride=(2, 2))\n",
        "        )\n",
        "        \n",
        "        self.block_3 = torch.nn.Sequential(        \n",
        "                torch.nn.Conv2d(in_channels=128,\n",
        "                                out_channels=256,\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(1, 1),\n",
        "                                padding=1),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.Conv2d(in_channels=256,\n",
        "                                out_channels=256,\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(1, 1),\n",
        "                                padding=1),\n",
        "                torch.nn.ReLU(),        \n",
        "                torch.nn.Conv2d(in_channels=256,\n",
        "                                out_channels=256,\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(1, 1),\n",
        "                                padding=1),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
        "                                   stride=(2, 2))\n",
        "        )\n",
        "        \n",
        "          \n",
        "        self.block_4 = torch.nn.Sequential(   \n",
        "                torch.nn.Conv2d(in_channels=256,\n",
        "                                out_channels=512,\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(1, 1),\n",
        "                                padding=1),\n",
        "                torch.nn.ReLU(),        \n",
        "                torch.nn.Conv2d(in_channels=512,\n",
        "                                out_channels=512,\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(1, 1),\n",
        "                                padding=1),\n",
        "                torch.nn.ReLU(),        \n",
        "                torch.nn.Conv2d(in_channels=512,\n",
        "                                out_channels=512,\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(1, 1),\n",
        "                                padding=1),\n",
        "                torch.nn.ReLU(),            \n",
        "                torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
        "                                   stride=(2, 2))\n",
        "        )\n",
        "        \n",
        "        self.block_5 = torch.nn.Sequential(\n",
        "                torch.nn.Conv2d(in_channels=512,\n",
        "                                out_channels=512,\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(1, 1),\n",
        "                                padding=1),\n",
        "                torch.nn.ReLU(),            \n",
        "                torch.nn.Conv2d(in_channels=512,\n",
        "                                out_channels=512,\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(1, 1),\n",
        "                                padding=1),\n",
        "                torch.nn.ReLU(),            \n",
        "                torch.nn.Conv2d(in_channels=512,\n",
        "                                out_channels=512,\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(1, 1),\n",
        "                                padding=1),\n",
        "                torch.nn.ReLU(),    \n",
        "                torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
        "                                   stride=(2, 2))             \n",
        "        )\n",
        "            \n",
        "        height, width = 3, 3                                      #initializing height and width of image\n",
        "        self.classifier = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512*height*width, 4096),              #Implmenting linear layer \n",
        "            torch.nn.ReLU(True),                                  #Relu= true for relu activation function\n",
        "            torch.nn.Dropout(p=0.5),                              #tO Ignore some neurons during training\n",
        "            torch.nn.Linear(4096, 4096),\n",
        "            torch.nn.ReLU(True),\n",
        "            torch.nn.Dropout(p=0.5),\n",
        "            torch.nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "        #Distributing data Uniformly \n",
        "            \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, torch.torch.nn.Conv2d) or isinstance(m, torch.torch.nn.Linear):\n",
        "                torch.nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    m.bias.detach().zero_()\n",
        "                    \n",
        "        self.avgpool = torch.nn.AdaptiveAvgPool2d((height, width))                 #AvgPool selects average prominent features. It will not take unimportant features.\n",
        "        \n",
        "        \n",
        "    def forward(self, x):                                             #forward method will use all of the layers we defined inside the constructor. In this way, the forward method explicitly defines the network's transformation.\n",
        "\n",
        "        x = self.block_1(x)                                      \n",
        "        x = self.block_2(x)\n",
        "        x = self.block_3(x)\n",
        "        x = self.block_4(x)\n",
        "        x = self.block_5(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1) # flatten\n",
        "        \n",
        "        logits = self.classifier(x)                                     #classifier to classify classes\n",
        "\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf7rD45j1KPe"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pazZYnC-gxI"
      },
      "source": [
        "model = VGG16(num_classes=10)                                       #limiting our model to 10 classes as our dataset has 10 classes \n",
        "\n",
        "model = model.to(device)                                            #assigning GPU to our model \n",
        "\n",
        "#Stochastic gradient descent is a optimizer to optimize training \n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(),                     \n",
        "                            momentum=0.9,                           #Simply momentum are jumps to find lowest minima. It improves training\n",
        "                            lr=0.01)                                #the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0.\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                                                       factor=0.1,\n",
        "                                                       mode='max',\n",
        "                                                       verbose=True)\n",
        "\n",
        "#Cross Entropy Loss to measure loss\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MD6EPb_X_PvJ"
      },
      "source": [
        "# In the validate() method, we are calculating the loss and accuracy. But we are not backpropagating the gradients. Backpropagation is only required during training.\n",
        "def validate(model, test_dataloader):\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    val_running_correct = 0\n",
        "    for int, data in enumerate(test_dataloader):\n",
        "        data, target = data[0].to(device), data[1].to(device)\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        \n",
        "        val_running_loss += loss.item()\n",
        "        _, preds = torch.max(output.data, 1)\n",
        "        val_running_correct += (preds == target).sum().item()\n",
        "    \n",
        "    val_loss = val_running_loss/len(test_dataloader.dataset)\n",
        "    val_accuracy = 100. * val_running_correct/len(test_dataloader.dataset)\n",
        "    \n",
        "    return val_loss, val_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWGoLJwNASyC"
      },
      "source": [
        "# implementing fit() method for training. We are calculating the gradients and backpropagating. \n",
        "\n",
        "def fit(model, train_dataloader):\n",
        "    model.train()\n",
        "    train_running_loss = 0.0\n",
        "    train_running_correct = 0\n",
        "    for i, data in enumerate(train_dataloader):\n",
        "        data, target = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        train_running_loss += loss.item()\n",
        "        _, preds = torch.max(output.data, 1)\n",
        "        train_running_correct += (preds == target).sum().item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    train_loss = train_running_loss/len(train_dataloader.dataset)\n",
        "    train_accuracy = 100. * train_running_correct/len(train_dataloader.dataset)\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}')\n",
        "    \n",
        "    return train_loss, train_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSxNzsXODY95"
      },
      "source": [
        "# We will train and validate the model for 10 epochs. All the while, both methods, the fit(), and validate() will keep on returning the loss and accuracy values for each epoch.\n",
        "#For each epoch, we will call the fit() and validate() method.\n",
        "\n",
        "train_loss , train_accuracy = [], []\n",
        "val_loss , val_accuracy = [], []\n",
        "start = time.time()\n",
        "for epoch in range(10):\n",
        "    train_epoch_loss, train_epoch_accuracy = fit(model, trainloader)\n",
        "    val_epoch_loss, val_epoch_accuracy = validate(model, testloader)\n",
        "    train_loss.append(train_epoch_loss)\n",
        "    train_accuracy.append(train_epoch_accuracy)\n",
        "    val_loss.append(val_epoch_loss)\n",
        "    val_accuracy.append(val_epoch_accuracy)\n",
        "end = time.time()\n",
        "print((end-start)/60, 'minutes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97agpMGpXdh8"
      },
      "source": [
        "Train Loss: 0.0256, Train Acc: 73.44\n",
        "\n",
        "Validation Loss: 0.0158, Validation Acc: 82.35\n",
        "\n",
        "Train Loss: 0.0148, Train Acc: 83.38\n",
        "\n",
        "Validation Loss: 0.0138, Validation Acc: 84.97\n",
        "\n",
        "Train Loss: 0.0117, Train Acc: 86.85\n",
        "\n",
        "Validation Loss: 0.0128, Validation Acc: 86.00\n",
        "\n",
        "Train Loss: 0.0094, Train Acc: 89.54\n",
        "\n",
        "Validation Loss: 0.0125, Validation Acc: 86.36\n",
        "\n",
        "Train Loss: 0.0074, Train Acc: 91.75\n",
        "\n",
        "Validation Loss: 0.0127, Validation Acc: 86.57\n",
        "\n",
        "Train Loss: 0.0057, Train Acc: 93.75\n",
        "\n",
        "Validation Loss: 0.0127, Validation Acc: 86.91\n",
        "\n",
        "Train Loss: 0.0043, Train Acc: 95.23\n",
        "\n",
        "Validation Loss: 0.0129, Validation Acc: 86.99\n",
        "\n",
        "Train Loss: 0.0032, Train Acc: 96.53\n",
        "\n",
        "Validation Loss: 0.0133, Validation Acc: 87.42\n",
        "\n",
        "Train Loss: 0.0023, Train Acc: 97.67\n",
        "\n",
        "Validation Loss: 0.0138, Validation Acc: 87.48\n",
        "\n",
        "Train Loss: 0.0018, Train Acc: 98.32\n",
        "\n",
        "Validation Loss: 0.0144, Validation Acc: 87.36\n",
        "\n"
      ]
    }
  ]
}